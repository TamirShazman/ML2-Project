\chapter*{Relationship of Optimal Encoders to the Prior}
The authors claim that when the encoders $q(z|x_{i}) = \frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}$ s.t. $\pi_{i} = \E_{\pi(z)}[\mathcal{I}(z\in\Omega_{i}]$, they are fixed points and, when optimized, form an equiprobable partition of the latent probability space. This in turn results in the marginal posterior being equal to the prior.

\section*{Proving Fixed Points}
Before we can show that $q(z|x)$ are fixed points, we must show that $\sigma^{2}$ approaches zero as the encoders and decoders are optimized.\\

\subsection*{1. $\sigma^2$ approaches zero}
\setcounter{equation}{0}
\begin{equation}
{ELBO} =  \E_{\rho(x)}\left[[\E_{q(z|x)}[\ln{p(x|z)}] - KL[q(z|x) \| \pi(z)]\right]
= \sum_{x}\rho(x)\left(\sum_{z} q(z|x) \ln{p(x|z)} - \sum_{z} ln \frac{q(z|x)}{\pi(z)}\right)
\end{equation}
We will note that the KL term is not dependent on $\sigma^2$. Therefore, we will ignore it as it will become zero in the partial derivative.
\begin{equation}
\frac{\partial}{\partial \sigma^2}\E_{\rho(x)q(z|x)}-\frac{1}{2}\left[\ln{2\pi}-\ln{\sigma^2}-\frac{\norm{x-g(z)}^2}{\sigma^2}\right]
\end{equation}
\begin{equation}
\E_{\rho(x)q(z|x)}\left[-\frac{1}{2}\left(-\frac{1}{\sigma^2}+\frac{\norm{x-g(z)}^2}{(\sigma^2)^2}\right)\right] 
\end{equation}
Setting the partial derivative to zero and noticing that $\sigma^{2}$ is not a random variable related to the expectation, we find:
\begin{equation}
\sigma^{2} = \E_{\rho(x)q(z|x)}\left[\norm{x-g(z)}^{2}\right]
\end{equation}

Therefore, as optimization brings g(z) closer to x, $\sigma^2$ approaches zero.

\subsection*{2. Simplifying g(z)}
As we saw earlier, $g(z)=\frac{\sum_{x}{q(z|x) x}}{\sum_{x}{q(z|x)}}$

We also previously mentioned that in their proofs the authors replace $q(z|x_{i})$ with $\frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}$ s.t. $\pi_{i} = \E_{\pi(z)}[\mathcal{I}(z\in\Omega_{i})]$\\
Let us note that assuming that $\lbrace{{\Omega_i}\rbrace}_{i=1}^n$ forms a partition of the latent probability space then:

\begin{gather*}
\forall x_i, \frac{q(z|x_i)}{\sum_j q(z|x_j)} = \frac{\frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}}{\sum_j \frac{\pi(z)\mathcal{I}({z\in\Omega_{j}})}{\pi_{j}}} = \frac{\frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}}{\frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}} = \mathcal{I}(z\in\Omega_i)
\end{gather*}

Therefore,
\begin{gather*}
g(z)=\frac{\sum_{i}{q(z|x_i) x_i}}{\sum_{i}{q(z|x_i)}} = \sum_i \mathcal{I}(z\in\Omega_i)x_i
\end{gather*}

\subsection*{3. $q(z|x)$ are fixed points}
Now we can continue to show that $q(z|x)$ are fixed points.\\

\begin{gather*}
q(z|x) \propto \pi(z)e^\frac{-\norm{x-g(z)}^2}{2\sigma^2}
\end{gather*}

Combining these equations and dividing by the necessary constant, we get:
\setcounter{equation}{0}
\begin{equation}
q(z|x_i) = \lim_{\sigma^2\to0}\frac{\pi(z)e^\frac{-\norm{x_i-g(z)}^2}{2\sigma^2}}{\sum_z\pi(z)e^\frac{-\norm{x_i-g(z)}^2}{2\sigma^2}}
\end{equation}
\begin{equation}
q(z|x_i) = \lim_{\sigma^2\to0}\frac{\pi(z)\sum_k e^\frac{-\norm{x_i- x_k}^2}{2\sigma^2}\mathcal{I}(z\in\Omega_k)}{\sum_z\pi(z)\sum_j e^\frac{-\norm{x_i- x_j}^2}{2\sigma^2}\mathcal{I}(z\in\Omega_j)}
\end{equation}
\begin{equation}
q(z|x_i) = \frac{\pi(z)\mathcal{I}(z\in\Omega_i)}{\pi_i}
\end{equation}
And this is a fixed-point for any prior $\pi(z)$ constrained by a partition such as $\lbrace{{\Omega_i}\rbrace}_{i=1}^n$

\section*{Proving Equiprobable Partition of the latent probability space}
Plugging these fixed points into the KL term we get:
\setcounter{equation}{0}
\begin{equation}
KL = \E_{\rho(x)q(z|x)}\left[\ln{\frac{q(z|x)}{\pi(z)}}\right] = \frac{1}{n} \sum_i \sum_z \frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}\ln{\frac{\pi(z)\mathcal{I}(z\in\Omega_i)}{\pi(z)*\pi_i}}
\end{equation}
\begin{equation}
= \frac{1}{n} \sum_i \sum_z \frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\sum_z \pi(z)\mathcal{I}(z\in\Omega_i)}\ln{\frac{\pi(z)\mathcal{I}(z\in\Omega_i)}{\pi(z)*\pi_i}}
\end{equation}
\begin{equation}
= \frac{1}{n} \sum_i \sum_z \frac{\mathcal{I}({z\in\Omega_{i}})}{\sum_z \mathcal{I}(z\in\Omega_i)}\ln{\frac{\mathcal{I}(z\in\Omega_i)}{\pi_i}}
\end{equation}
\begin{equation}
= \frac{1}{n} \sum_i \ln{\frac{1}{\pi_i}} = -\frac{1}{n} \sum_i \ln{\pi_i}
\end{equation}
Solving for the maximal values of $\pi_i$ using the Lagrangian with the usual constraints that assume that $\pi_i$ a probability, we find:
\begin{equation}
-\frac{1}{n}\sum_i\ln{\pi_i} +\eta(\sum_i\pi_i-1)-\sum_i\lambda_i^1\pi_i +\sum_i\lambda_i^2(\pi_i-1)
\end{equation}
For some $i$
\begin{equation}
\frac{\partial}{\partial \pi_i}\left(-\frac{1}{n}\sum_i\ln{\pi_i} +\eta(\sum_i\pi_i-1)-\sum_i\lambda_i^1\pi_i +\sum_i\lambda_i^2(\pi_i-1)\right)= \frac{1}{n*\pi_i} + \eta - \lambda_i^1 + \lambda_i^2
\end{equation}
When setting the partial derivative to zero and assuming that $\pi_i \neq 0$ or $1$ we get
\begin{equation}
\frac{1}{n*\eta} = \pi_i
\end{equation}
Using the condition that $\sum_i\pi_i=1$,
\begin{equation}
\sum_i\frac{1}{n*\eta} = \sum_i\pi_i
\end{equation}
\begin{equation}
\frac{1}{\eta} = 1
\end{equation}
Therefore, $\eta = 1$ and $\forall i, \pi_i = \frac{1}{n}$

We remind the reader that $\pi_{i} = \E_{\pi(z)}[\mathcal{I}(z\in\Omega_{i})]$, which is to say the proportion of z in $\Omega_i$. Therefore, if $\forall i, \pi_i = \frac{1}{n}$ then the z are equally distributed between the partition $\lbrace{{\Omega_i}\rbrace}_{i=1}^n$.

\section*{Marginal Posterior Equal to Prior}
In light of the previous mathematical developments, we look at the marginal posterior $q(z)$ given a partition $\lbrace{{\Omega_i}\rbrace}_{i=1}^n$.
\begin{gather*}
g(z) = \frac{1}{n}\sum_i q(z|x_i) = \frac{1}{n}\sum_i \frac{\pi(z)\mathcal{I}(z\in\Omega_i)}{\pi_i} = \frac{n * \pi(z)}{n} \sum_i \mathcal{I}(z\in\Omega_i)= \pi(z)
\end{gather*}
Therefore, optimal solutions for VAE's encoders are inference models that cover the latent space in such a way that their marginal is equal to the prior.

\section*{Conclusion}
The authors show that $q(z|x_{i}) = \frac{\pi(z)\mathcal{I}({z\in\Omega_{i}})}{\pi_{i}}$ s.t. $\pi_{i} = \E_{\pi(z)}[\mathcal{I}(z\in\Omega_{i})]$ are fixed points and upon convergence to these fixed points, ELBO is maximized when the marginal posterior equals the prior.





