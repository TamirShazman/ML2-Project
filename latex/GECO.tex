\chapter*{The GECO Algorithm}
The main contribution of the authors in this paper is the GECO algorithm.\\
After explaining the "holes" and blurred reconstruction phenomena and enumerating the qualities of the $\beta$-VAE, the authors offer a principled approach to controlling the balance between the reconstruction error and the KL divergence.\\
Unlike $\beta$-VAE in which the machine learning practitioner must search for an elusive, optimal $\beta$ to control the role of the unintuitive KL term, GECO optimizes the trade-off during training.
\subsection*{GECO Algorithm}
Initialize $t = 0$;\\
Initialize $\lambda = 1$;\\
\textbf{while} \textit{training} \textbf{do}:
\renewenvironment{description}[1][0pt]
  {\list{}{\labelwidth=0pt \leftmargin=#1
   \let\makelabel\descriptionlabel}}
  {\endlist}
\begin{description}[1cm]
\item Process current batch, x;
\item Sample from variational posterior $z\sim{q(z|x)}$
\item Compute empirical expectation of the reconstruction error $C^t = \mathcal{C}(x^t,g(z^t))$;
\item \textbf{if t is 0} then initialize the moving average of the reconstruction error, $C_{ma}^0 = C^0$
\item \textbf{else} $C_{ma}^t = \alpha C_{ma}^{t-1} + (1-\alpha)C^t$;
\item $C^t + StopGradient(C_{ma}^t - C^t)$
\item Update the parameters of the encoders and decoders using gradient descent
\item Update $\lambda$ using the update rule $\lambda^t = \lambda^{t-1}e^{\propto C^t}$
\item $t = t+1$
\end{description}

As stated earlier, the GECO algorithm optimizes the trade-off parameter that controls the relative importance of the KL term and the reconstruction error.\\
Additionally, by using $e$, $\lambda$ stays a positive number for all updates. This is a necessary requirement of optimization when using the Lagrangian, the mathematical basis of GECO. \\
The GECO algorithm allows the trade-off to adapt during training. For example, if the reconstruction error is high then more weight may be given to the reconstruction error; thereby, minimizing it.\\
This is important because often VAEs achieve the necessary reconstruction error, i.e. no noticeable blurriness, early on in training. After that point, when they continue to reduce the total error, these VAEs minimize the reconstruction error at the expense of the KL term.\\
The authors show the significance of this in their empirical trials using ConvDraw trained on the CelebA and Color-MNIST data sets.\\
Below, we can see that using ELBO creates a latent space with blurriness and holes-results of the unnecessary reduction of the KL term in favor of the reconstruction error. In contrast, GECO achieves reconstruction that is visually equivalent with no holes or blurriness.\\

\begin{figure}[H]
\includegraphics[width=\textwidth]{MNIST}
\caption{Note the differences in the last column of the ELBO dataset}
\centering
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{CelebA}
\caption{Note the differences in the last column of the ELBO dataset}
\centering
\end{figure}
\hfill\break

The authors show with the following graphs how GECO succeeds in maintaining low KL during training.
\begin{figure}[H]
\includegraphics[width=\textwidth]{PapersInfoPlane}
\centering
\end{figure}

We notice that the circles, representing GECO, are always farthest to the left. GECO has the lowest KL term.\\
In conjunction with the pictures presented earlier, we conclude that GECO achieves well reconstructed images \textbf{and} low KL divergence.\\

Additionally, the authors present the following graph, showing the steady decrease of KL for GECO relative to ELBO.
\begin{figure}[H]
\includegraphics[width=\textwidth]{PapersKLgraph}
\centering
\end{figure}



	